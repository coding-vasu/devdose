# Ollama Setup for DevDose Pipeline

## âœ… What's Been Done

### 1. Installed Ollama

- âœ… Installed via Homebrew
- âœ… Started Ollama service
- â³ Downloading Llama 3.2 (3B) model (~2GB, 12% complete)

### 2. Updated Pipeline Code

- âœ… Installed `ollama` NPM package
- âœ… Created `ollama-client.ts` - Local AI client
- âœ… Updated `processing/index.ts` to use Ollama instead of Gemini
- âœ… Configured for smaller batch sizes (5 instead of 10) for local processing

### 3. Model Configuration

- **Model:** `llama3.2:3b`
- **Size:** 2GB
- **Speed:** Fast (runs on your Mac)
- **Cost:** $0 - Completely free!

---

## ğŸ¯ Next Steps

### Once Model Download Completes (~12 minutes):

1. **Test the Pipeline:**

   ```bash
   npm run test-pipeline
   ```

2. **Expected Results:**
   - âœ… 10 code snippets processed with AI
   - âœ… Titles and explanations generated
   - âœ… Quality scoring completed
   - âœ… Posts enriched with metadata
   - âœ… Published to Supabase database

---

## ğŸš€ How It Works

### Ollama vs Gemini

| Feature      | Gemini (Cloud)      | Ollama (Local)    |
| ------------ | ------------------- | ----------------- |
| **Cost**     | Free tier (limited) | 100% Free         |
| **Speed**    | Fast (API call)     | Fast (local)      |
| **Privacy**  | Data sent to Google | 100% Private      |
| **Internet** | Required            | Not required      |
| **Setup**    | API key needed      | One-time download |

### Pipeline Flow with Ollama

```
Code Snippet â†’ Ollama (Local AI) â†’ Generated Content
                â†“
          Title, Explanation,
          Difficulty, Tags,
          Quality Score
```

---

## ğŸ“Š Performance Expectations

### Processing Speed (Local)

- **Single snippet:** ~2-5 seconds
- **10 snippets:** ~30-60 seconds
- **100 snippets:** ~5-10 minutes

### Quality

- Llama 3.2 is a high-quality model
- Comparable to GPT-3.5
- Excellent for code explanation

---

## ğŸ”§ Commands Reference

### Ollama Commands

```bash
# Check if Ollama is running
brew services list | grep ollama

# List downloaded models
ollama list

# Pull a different model
ollama pull llama3.2:1b  # Smaller, faster
ollama pull llama3.2:7b  # Larger, better quality

# Test Ollama directly
ollama run llama3.2:3b "Explain this code: const x = 5;"

# Stop Ollama service
brew services stop ollama
```

### Pipeline Commands

```bash
# Test pipeline with sample data
npm run test-pipeline

# Run full pipeline
npm run pipeline

# Run individual stages
npm run processing
npm run quality
npm run enrichment
npm run publishing
```

---

## ğŸ’¡ Tips

### 1. **Model Selection**

- `llama3.2:1b` - Fastest, smallest (1GB)
- `llama3.2:3b` - **Recommended** - Good balance (2GB)
- `llama3.2:7b` - Best quality, slower (4GB)

### 2. **Batch Size**

Currently set to 5 snippets per batch. Adjust in `.env`:

```bash
BATCH_SIZE=5  # Smaller = slower but more stable
BATCH_SIZE=10 # Faster but uses more RAM
```

### 3. **Memory Usage**

- Llama 3.2 (3B) uses ~4GB RAM
- Make sure you have at least 8GB total RAM
- Close other apps if needed

---

## ğŸ› Troubleshooting

### Model Download Stuck?

```bash
# Cancel and restart
pkill ollama
brew services restart ollama
ollama pull llama3.2:3b
```

### "Connection refused" Error?

```bash
# Make sure Ollama is running
brew services start ollama

# Check status
curl http://localhost:11434
```

### Out of Memory?

```bash
# Use smaller model
ollama pull llama3.2:1b

# Update pipeline to use it
# Edit src/pipeline/processing/index.ts
# Change: model: "llama3.2:1b"
```

---

## ğŸ‰ Benefits of Ollama

1. **âœ… No API Keys** - No setup hassle
2. **âœ… Unlimited Usage** - Process as much as you want
3. **âœ… Privacy** - Your code never leaves your computer
4. **âœ… Offline** - Works without internet
5. **âœ… Fast** - No network latency
6. **âœ… Free** - Zero cost forever

---

## ğŸ“ What's Next?

Once the model finishes downloading:

1. Run `npm run test-pipeline`
2. Check the results in Supabase
3. If it works, run the full pipeline with real data
4. Celebrate! ğŸ‰

---

**Current Status:** â³ Downloading model... (12% complete, ~12 minutes remaining)

**Check progress:**

```bash
# In another terminal
watch -n 5 'ollama list'
```
